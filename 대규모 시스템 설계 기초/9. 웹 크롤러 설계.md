# 9. 웹 크롤러 설계
### 크롤러의 사용처
- 검색 엔진 인덱싱
- 웹 아카이빙
- 웹 마이닝
- 웹 모니터링
<br/>

## 1단계. 문제 이해 및 설계 범위 확정
### 기본 기능
1. url 집합이 입력으로 주어지면, 해당 Url이 가리키는 모든 웹 페이지를 다운로드 한다.
2. 다운 받은 웹페이지에서 url들을 추출한다.
3. 추출된 Url에서 목록에 추가하고 위의 과정을 처음부터 반복한다.
<br/>

### 질문을 통한 모호함 제거 
웹 크롤러가 처리해야 하는 데이터의 규모에 따라 웹 크롤러의 복잡도가 달라진다.
1.크롤러의 주된 용도가 무엇인지? 검색 엔진 인덱스에 쓰인다.
2. 매달 얼마나 많은 웹페이지를 수집해야 하는지? 10억 개의 웹 페이지를 수집해야 한다.
3. 새로 만들어진 웹 페이지나 수정된 웹 페이지도 고려해야 하는지? 그렇다.
4. 수정한 웹 페이지는 저장해야 하는지? 5년간 저장
5. 중복된 콘텐츠는 어떻게 해야 하는가? 중복된 콘텐츠를 갖는 페이지는 무시

### 설계 고려 사항
크롤러는 규모확장성, 안정성, 예절, 확장성을 만족해야 한다.
- 규모 확장성 : 병행성을 이용하면 효과적으로 수행 가능
- 안정성 : 악성 코드가 붙은 링크에 대응해야 한다.
- 예절 : 짧은 시간동안 너무 많은 요청을 보내면 안된다.
- 확장성 : 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다. 이미지 파일도 크롤링하고 싶을 때 전체 시스템을 새로 설계하지 않아도 되도록.
<br/>

### 개략적 규모 추정
- 매달 10억 개의 웹 페이지를 다운로드
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 page / sec
- 최대 (Peak) QPS = 2 X QPS = 800 page/sec
- 웹 페이지의 평균 크기는 500k라고 가정
- 10억 페이지 x 500k = 500 TB /month.
- 1개월치 데이터를 보관하는 데는 500 TB, 5년간 보관한다고 가정하면 결국 500TB X 12개월 X 5년 = 30PB의 저장용량이 필요하다.
<br/>


## 2단계. 개략적 설계안 제시 및 동의 구하기
아래 다이어그램에 등작하는 컴포넌트들의 동작을 살펴보자.
<img width="413" alt="image" src="https://github.com/Beakjiyeon/reading-record/assets/35768650/8087a246-4bbf-4b99-bed1-5d3385304727">
### 시작 URL 집합
### 미수집 URL 저장소
### HTML 다운로더
### 도메인 이름 변환기
### 콘텐츠 파서
### 중복 콘텐츠인가?
### 콘텐츠 저장소
### URL 추출기
### URL 필터
### 이미 방뭄ㄴ한 URL인가?
### URL 저장소

### 웹 클로러 작업 흐름
1. 시작 url들을 미수집 url 저장소에 저장한다.
2. html 다운로더는 미수집 url 저장소에서 url 목록을 가져온다.
3. html 다운로더는 도메인 이름 변환기를 사용하여 url의 Ip 주소를 알아내고, 해당 Ip 주소로 접속하여 웹페이지를 다운받는다.
4. 콘텐츠 파서는 다운된 html 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시한다.
6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.
   - 이미 저장소에 있는 콘텐츠인 경우에는 처리하지 않고 버린다.
   - 저장소에 없는 콘텐츠인 경우에는 저장소에 저장한 뒤 url 추출기로 전달한다.
7. url 추출기는 해당 html 페이지에서 링크를 골라낸다.
8. 골라낸 링크를 Url 필터로 전달한다.
9. 필터링이 끝나고 남은 url만 중복 Url 판별 단계로 전달한다.
10. 이미 처리한 url인지 확인하기 위하여, url 저장소에 보관된 Url인지 살핀다. 이미 저장소에 있는 Url은 버린다.
11. 저장소에 없는 url은 url 저장소에 저장할 뿐 아니라 미수집 Url 저장소에도 전달한다.

<br/>

## 3단계. 상세 설계
## 4단계. 마무리


### 참고
크롤러 스레드 중 하나가 DNS 요청을 보낼 때 다른 스레드의 DNS 요청이 전부 블록되는 이유는 크롤러가 사용하는 DNS 캐시와 DNS 서버의 작동 방식에 관련이 있습니다.

일반적으로, DNS(Domain Name System)은 도메인 이름을 IP 주소로 해석해주는 역할을 합니다. DNS 요청을 보내면 DNS 서버에서 해당 도메인 이름에 대한 IP 주소를 찾아 응답을 반환합니다. 그러나 DNS 서버는 대량의 요청을 처리하기 위해 일정한 처리 시간이 필요하며, 이를 위해 DNS 서버는 요청을 순차적으로 처리하는 큐 형태의 구조를 가지고 있을 수 있습니다.

따라서, 하나의 크롤러 스레드가 DNS 요청을 보내면 DNS 서버는 이 요청을 큐에 추가하고 처리를 시작합니다. 이때 다른 크롤러 스레드가 동시에 DNS 요청을 보내면, 이 요청들은 순차적으로 처리를 기다려야 합니다. 처리 중인 요청이 완료되기 전에는 다른 요청들은 블록되어 대기하게 되는 것입니다.

따라서, 하나의 스레드가 DNS 요청을 보내면 다른 스레드의 DNS 요청은 블록되는 현상이 발생할 수 있습니다. 이는 DNS 서버의 처리 방식과 관련이 있으며, 동시에 처리할 수 있는 요청의 수를 늘리기 위해 DNS 서버의 구성을 조정하거나 대안적인 DNS 서버를 고려해볼 수 있습니다.



<br/>

ChatGPT
Spring Boot는 기본적으로 Java의 DNS resolver를 사용하며, Java의 DNS resolver는 동기적으로 동작합니다. 이는 Spring Boot의 기본 설정이며, 호출 스레드가 DNS 응답을 받을 때까지 블로킹됩니다.

그러나 Spring Boot는 비동기식 DNS resolver를 구성하는 옵션도 제공합니다. springboot-starter-web 의존성을 추가하고 application.properties 파일에 spring.webclient.use-blocking-dns=false 설정을 추가하면 비동기식 DNS resolver가 사용됩니다. 이렇게 설정하면 DNS 요청은 백그라운드 스레드 풀에서 비동기적으로 처리되므로 호출 스레드는 블로킹되지 않고 다른 작업을 수행할 수 있습니다.

따라서 Spring Boot에서 DNS resolver를 비동기식으로 동작하도록 구성할 수 있지만, 기본적으로는 동기적으로 동작합니다. 설정을 변경하여 비동기식 DNS resolver를 사용하려면 위에 언급한 설정을 추가하면 됩니다.


아까… 어떤분이 말씀해주신것 같긴 한데
언어 / 프레임워크 구현 방법에 따라 다른것 같습니다
일단 java의 DNS resolver는 동기적으로 동작한다고 합니다
